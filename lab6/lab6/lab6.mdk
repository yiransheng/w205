Title         : W205 Lab 6

Author        : Yiran Sheng
Affiliation   : UCB
Email         : yiran@ischool.berkeley.edu

Heading Base  : 2
Bib style     : plainnat
Doc class     : [reprint,nocopyrightspace]style/sigplanconf.cls

[TITLE]

~ Abstract
W205 Lab 6. An	Introduction	to	Apache	Spark	and	Spark	SQL. Spark is setup on a cluster of three nodes, and running on Yarn. 
~

~ TexRaw
% any commands necessary for your particular style
\category{D.2.5}{Software Engineering}{Testing and Debugging}[symbolic execution]
\terms{Algorithms, Experimentation}
\keywords{Games for learning, white box testing}
~

# Step 0.1	Check	Installation	and	Prepare	Data

There seems to be a problem with AMI ucbw205_complete_plus_postgres (ami-bff58fda), where Spark is not configured properly. 

```
[root@ip-10-0-0-61 ~]# echo $SPARK_HOME
/root/spark15
```

Cause is in `/etc/profile` there's a weird line:
```
export SPARK_HOME=$HOME/spark15  
```

After commenting it out, everything seemed fine. Next adding spark `bin/` directory to `PATH` variable. 

```
cat <<EOF >> /root/.bash_profile
export PATH=$SPARK_HOME/bin:$PATH
EOF  
```

# Step 0.2 Check Spark on yarn client mode 

I have setup a three-ec2-instance cluster from base ami, in this step try to check if spark runs properly on yarn. 

First, check if yarn nodemanager and resourcemanager are running. 

```
[root@ip-10-0-0-61 ~]# yarn node -list
15/10/09 19:48:14 INFO client.RMProxy: Connecting to ResourceManager at master/10.0.0.61:8032
Total Nodes:3
         Node-Id	     Node-State	Node-Http-Address	Number-of-Running-Containers
    slave1:56898	        RUNNING	      slave1:8042	                           0
    master:52195	        RUNNING	      master:8042	                           0
    slave2:44917	        RUNNING	      slave2:8042	                           0
[root@ip-10-0-0-61 ~]# 
  
```

Next, start `spark-shell` in `yarn-client` mode

```
[root@ip-10-0-0-61 ~]# spark-shell --master yarn-client  
...
15/10/09 19:50:15 INFO yarn.Client: Application report for application_1444418436488_0003 (state: ACCEPTED)
...
```

Logs indiate `yarn` has started a master container for spark. And spark shell is successfully loaded.

```
scala>  
```
List of yarn applications produces:

```
[root@ip-10-0-0-59 ~]# yarn application -list
15/10/09 20:02:48 INFO client.RMProxy: Connecting to ResourceManager at master/10.0.0.61:8032
Total number of applications (application-types: [] and states: [SUBMITTED, ACCEPTED, RUNNING]):1
                Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL
application_1444418436488_0004	         Spark shell	               SPARK	      root	 root.root	           RUNNING	         UNDEFINED	            10%	                 http://master:4040  
```
Confirms spark is running on yarn. 

# Step 0.3 Download Data and copy to hdfs

Clone github and copy file to hdfs.
```
[root@ip-10-0-0-61 ~]# git clone : https://github.com/UC-Berkeley-ISchool/w205-labs-exercises/tree/master/data/Crimes_-_2001_to_present_data
[root@ip-10-0-0-61 ~]# cd w205-labs-exercises/data/Crimes_-_2001_to_present_data/ 
[root@ip-10-0-0-61 ~]# gunzip Crimes_-_2001_to_present.csv.gz
[root@ip-10-0-0-61 ~]# hdfs dfs -mkdir w205lab6
[root@ip-10-0-0-61 ~]# hdfs dfs -copyFromLocal Crimes_-_2001_to_present_data.csv w205lab6/Crimes_-_2001_to_present_data.csv
[root@ip-10-0-0-61 ~]# wget https://s3.amazonaws.com/ucbdatasciencew205/labs/weblog_lab.csv -O - | hdfs dfs -put - w205lab6/weblog_lab.csv
[root@ip-10-0-0-61 ~]# hdfs dfs -ls w205lab6
```

Output:
```
Found 2 items
-rw-r--r--   1 root supergroup 1376617753 2015-10-09 20:35 w205lab6/Crimes_-_2001_to_present.csv
-rw-r--r--   1 root supergroup    5192992 2015-10-09 20:40 w205lab6/weblog_lab.csv

```

# Step 1.	Start	pyspark

Some basic pyspark examples:
```
>>> x = [1,2,3,4,5,6,7,8,9];
>>> distData = sc.parallelize(x)
>>> print(distData)
ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:376
>>> nx=distData.count() 
15/10/09 20:53:32 INFO spark.SparkContext: Starting job: count at <stdin>:1
15/10/09 20:53:33 INFO scheduler.DAGScheduler: Got job 0 (count at <stdin>:1) with 1 output partitions (allowLocal=false)
15/10/09 20:53:33 INFO scheduler.DAGScheduler: Final stage: Stage 0(count at <stdin>:1)
15/10/09 20:53:33 INFO scheduler.DAGScheduler: Parents of final stage: List()
15/10/09 20:53:33 INFO scheduler.DAGScheduler: Missing parents: List()
15/10/09 20:53:33 INFO scheduler.DAGScheduler: Submitting Stage 0 (PythonRDD[1] at count at <stdin>:1), which has no missing parents
15/10/09 20:53:33 INFO storage.MemoryStore: ensureFreeSpace(4192) called with curMem=0, maxMem=280248975
15/10/09 20:53:33 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.1 KB, free 267.3 MB)
15/10/09 20:53:33 INFO storage.MemoryStore: ensureFreeSpace(2709) called with curMem=4192, maxMem=280248975
15/10/09 20:53:33 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.6 KB, free 267.3 MB)
15/10/09 20:53:33 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:33651 (size: 2.6 KB, free: 267.3 MB)
15/10/09 20:53:33 INFO storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
15/10/09 20:53:33 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:839
15/10/09 20:53:33 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 0 (PythonRDD[1] at count at <stdin>:1)
15/10/09 20:53:33 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/10/09 20:53:33 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1282 bytes)
15/10/09 20:53:33 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
15/10/09 20:53:34 INFO python.PythonRDD: Times: total = 1036, boot = 1017, init = 19, finish = 0
15/10/09 20:53:35 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 698 bytes result sent to driver
15/10/09 20:53:35 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1284 ms on localhost (1/1)
15/10/09 20:53:35 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/10/09 20:53:35 INFO scheduler.DAGScheduler: Stage 0 (count at <stdin>:1) finished in 1.399 s
15/10/09 20:53:35 INFO scheduler.DAGScheduler: Job 0 finished: count at <stdin>:1, took 2.228150 s
>>> print nx
9  
```

# Step 2.	Load	a	File	and	Count	the	Rows

We run `pyspark` in yarn-client mode.
```
[root@ip-10-0-0-61 ~]# pyspark --master yarn-client  
```
```
# csv file is read from hdfs
>>> crimedata = sc.textFile("hdfs:///user/root/w205lab6/Crimes_-_2001_to_present.csv")
>>> crimedata.count()                                                                 
[Stage 0:>                                                         (0 + 2) / 11]

5862796
>>> crimedata.first()
u'ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location'
>>> crimedata.take(10)    
[u'ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location', u'10184515,HY372204,08/06/2015 11:55:00 PM,033XX W DIVERSEY AVE,2027,NARCOTICS,POSS: CRACK,STREET,true,false,1412,014,35,22,18,1153440,1918377,2015,08/13/2015 12:57:42 PM,41.931870591,-87.711546895,"(41.931870591, -87.711546895)"', u'10184607,HY372206,08/06/2015 11:55:00 PM,035XX S RHODES AVE,0486,BATTERY,DOMESTIC BATTERY SIMPLE,APARTMENT,false,true,0212,002,4,35,08B,1180132,1881331,2015,08/13/2015 12:57:42 PM,41.82964147,-87.614598779,"(41.82964147, -87.614598779)"', u'10190430,HY374464,08/06/2015 11:50:00 PM,047XX W HARRISON ST,0430,BATTERY,AGGRAVATED: OTHER DANG WEAPON,GAS STATION,false,false,1131,011,24,25,04B,1144626,1896881,2015,08/13/2015 12:57:42 PM,41.873054046,-87.744479572,"(41.873054046, -87.744479572)"', u'10185476,HY372534,08/06/2015 11:50:00 PM,030XX W FLETCHER ST,0620,BURGLARY,UNLAWFUL ENTRY,RESIDENCE-GARAGE,false,false,1411,014,33,21,05,1155716,1920830,2015,08/13/2015 12:57:42 PM,41.938556204,-87.703116637,"(41.938556204, -87.703116637)"', u'10184561,HY372224,08/06/2015 11:50:00 PM,034XX S RACINE AVE,0820,THEFT,$500 AND UNDER,PARKING LOT/GARAGE(NON.RESID.),true,false,0913,009,11,60,06,1168866,1881886,2015,08/13/2015 12:57:42 PM,41.831415654,-87.655917306,"(41.831415654, -87.655917306)"', u'10184518,HY372189,08/06/2015 11:45:00 PM,038XX S HONORE ST,1310,CRIMINAL DAMAGE,TO PROPERTY,RESIDENCE,false,false,0912,009,11,59,14,1164617,1879167,2015,08/13/2015 12:57:42 PM,41.824045312,-87.671584096,"(41.824045312, -87.671584096)"', u'10184620,HY372195,08/06/2015 11:45:00 PM,011XX N WELLS ST,0810,THEFT,OVER $500,TAXICAB,false,false,1824,018,43,8,06,1174556,1907722,2015,08/13/2015 12:57:42 PM,41.902186365,-87.634268385,"(41.902186365, -87.634268385)"', u'10184573,HY372193,08/06/2015 11:44:00 PM,012XX W 82ND ST,0460,BATTERY,SIMPLE,SIDEWALK,false,false,0613,006,21,71,08B,1169200,1850440,2015,08/13/2015 12:57:42 PM,41.74511691,-87.655601328,"(41.74511691, -87.655601328)"', u'10184606,HY372191,08/06/2015 11:40:00 PM,048XX S INDIANA AVE,0486,BATTERY,DOMESTIC BATTERY SIMPLE,ALLEY,false,true,0224,002,3,38,08B,1178377,1872994,2015,08/13/2015 12:57:42 PM,41.80680416,-87.621291256,"(41.80680416, -87.621291256)"']
>>> noHeaderCrimedata = crimedata.zipWithIndex().filter(lambda (row,index): index >0).keys()
>>> noHeaderCrimedata.first()                                                   
u'10184515,HY372204,08/06/2015 11:55:00 PM,033XX W DIVERSEY AVE,2027,NARCOTICS,POSS: CRACK,STREET,true,false,1412,014,35,22,18,1153440,1918377,2015,08/13/2015 12:57:42 PM,41.931870591,-87.711546895,"(41.931870591, -87.711546895)"'
```
# Step 3.	Filter Records and	Structures

```
>>> narcoticsCrimes = noHeaderCrimedata.filter(lambda x:"NARCOTICS" in x)
>>> narcoticsCrimes.count()
663712                                                                          
>>> narcoticsCrimeRecords = narcoticsCrimes.map(lambda r :r.split(","))
>>> narcoticsCrimeRecords.first()
[u'10184515', u'HY372204', u'08/06/2015 11:55:00 PM', u'033XX W DIVERSEY AVE', u'2027', u'NARCOTICS', u'POSS: CRACK', u'STREET', u'true', u'false', u'1412', u'014', u'35', u'22', u'18', u'1153440', u'1918377', u'2015', u'08/13/2015 12:57:42 PM', u'41.931870591', u'-87.711546895', u'"(41.931870591', u' -87.711546895)"']  
```

# Step 4.	Key Values

```
>>> narcoticsCrimeRecords = narcoticsCrimes.map(lambda r :r.split(","))
>>> narcoticsCrimeRecords.first()
[u'10184515', u'HY372204', u'08/06/2015 11:55:00 PM', u'033XX W DIVERSEY AVE', u'2027', u'NARCOTICS', u'POSS: CRACK', u'STREET', u'true', u'false', u'1412', u'014', u'35', u'22', u'18', u'1153440', u'1918377', u'2015', u'08/13/2015 12:57:42 PM', u'41.931870591', u'-87.711546895', u'"(41.931870591', u' -87.711546895)"']
>>> narcoticsCrimeTuples = narcoticsCrimes.map(lambda x:(x.split(",")[0], x))
>>> narcoticsCrimeTuples = narcoticsCrimes.map(lambda x:(x.split(",")[0], x))
>>> sorted=narcoticsCrimeTuples.sortByKey()
>>> sorted.take(10)                          
```
First 10 records of sorted narcotics crime (pretty printed as json, tuples are coresed into arrays):
```
>>> print( json.dumps(sorted.take(10), indent=4, separators=(',', ': ')) )
[
    [
        "10000014",
        "10000014,HY189846,03/18/2015 05:52:00 PM,107XX S EGGLESTON AVE,1822,NARCOTICS,MANU/DEL:CANNABIS OVER 10 GMS,RESIDENCE,true,false,2233,022,34,49,18,1175162,1833372,2015,03/25/2015 12:42:30 PM,41.698149046,-87.634263523,\"(41.698149046, -87.634263523)\""
    ],
    [
        "10000015",
        "10000015,HY190002,03/18/2015 10:00:00 PM,001XX S SPRINGFIELD AVE,2027,NARCOTICS,POSS: CRACK,SIDEWALK,true,false,1122,011,28,26,18,1150447,1899324,2015,03/25/2015 12:42:30 PM,41.879646339,-87.723043936,\"(41.879646339, -87.723043936)\""
    ],
    [
        "10000018",
        "10000018,HY189982,03/18/2015 09:20:00 PM,018XX E 75TH ST,2028,NARCOTICS,POSS: SYNTHETIC DRUGS,STREET,true,false,0324,003,8,43,18,1189965,1855654,2015,03/25/2015 12:42:30 PM,41.758950606,-87.579348528,\"(41.758950606, -87.579348528)\""
    ],
    [
        "10000032",
        "10000032,HY190005,03/18/2015 10:00:00 PM,078XX S KEDZIE AVE,2024,NARCOTICS,POSS: HEROIN(WHITE),STREET,true,false,0835,008,18,70,18,1156360,1852673,2015,03/25/2015 12:42:30 PM,41.751512505,-87.702589481,\"(41.751512505, -87.702589481)\""
    ],
    [
        "10000033",
        "10000033,HY189981,03/18/2015 09:45:00 PM,032XX W 23RD ST,1811,NARCOTICS,POSS: CANNABIS 30GMS OR LESS,SIDEWALK,true,false,1024,010,22,30,18,1154889,1888566,2015,03/25/2015 12:42:30 PM,41.850037381,-87.707021653,\"(41.850037381, -87.707021653)\""
    ],
    [
        "10000036",
        "10000036,HY189959,03/18/2015 09:37:00 PM,079XX S LOOMIS BLVD,1811,NARCOTICS,POSS: CANNABIS 30GMS OR LESS,STREET,true,false,0612,006,21,71,18,1168414,1852212,2015,03/25/2015 12:42:30 PM,41.749996475,-87.658430431,\"(41.749996475, -87.658430431)\""
    ],
    [
        "10000038",
        "10000038,HY189993,03/18/2015 10:00:00 PM,055XX N WESTERN AVE,1811,NARCOTICS,POSS: CANNABIS 30GMS OR LESS,STREET,true,false,2012,020,40,4,18,1159361,1936925,2015,03/25/2015 12:42:30 PM,41.982647434,-87.689275782,\"(41.982647434, -87.689275782)\""
    ],
    [
        "10000049",
        "10000049,HY190034,03/18/2015 10:45:00 PM,003XX N WALLER AVE,1811,NARCOTICS,POSS: CANNABIS 30GMS OR LESS,SIDEWALK,true,false,1512,015,29,25,18,1138053,1901603,2015,03/25/2015 12:42:30 PM,41.886133043,-87.768498451,\"(41.886133043, -87.768498451)\""
    ],
    [
        "10000050",
        "10000050,HY190024,03/18/2015 09:55:00 PM,015XX S CALIFORNIA BLVD,2093,NARCOTICS,FOUND SUSPECT NARCOTICS,HOSPITAL BUILDING/GROUNDS,true,false,1022,010,12,29,26,1157891,1892607,2015,03/25/2015 12:42:30 PM,41.861065709,-87.695893691,\"(41.861065709, -87.695893691)\""
    ],
    [
        "10000058",
        "10000058,HY190039,03/18/2015 10:38:00 PM,120XX S LAFAYETTE AVE,1811,NARCOTICS,POSS: CANNABIS 30GMS OR LESS,RESIDENTIAL YARD (FRONT/BACK),true,false,0523,005,9,53,18,1178055,1825033,2015,03/25/2015 12:42:30 PM,41.675200673,-87.623922325,\"(41.675200673, -87.623922325)\""
    ]
]  
```
# Step 5.	Start	Spark SQL


