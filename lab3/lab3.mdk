Title         : W205 Lab3
Author        : Yiran Sheng
Affiliation   : Research institute
Email         : yiran@ischool.berkeley.edu

Colorizer     : shell
Bib style     : plainnat
Bibliography  : example
Heading base  : 2

Doc class     : [10pt]article

[TITLE]

~ Abstract
W205 Lab3, Hive. 
~


# Questions

## Q1 : Where data for a Managed table is stored?

The following command produces the location where a given managed table is stored.

```
hive -S -e "describe formatted <table_name> ;" | grep 'Location' | awk '{ print $NF }'
```

For example for Lab3, to find out where table `Web_Session_Log` is stored:

```
> hive -S -e "describe formatted Web_Session_Log ;" | grep 'Location' | awk '{ print $NF }'  
file:/user/hive/warehouse/web_session_log_rc  
```

**Reference:** [http://stackoverflow.com/questions/5058400/where-does-hive-store-its-files-in-hdfs](http://stackoverflow.com/questions/5058400/where-does-hive-store-its-files-in-hdfs)

## Q2 : Will the data for an external table drop if the table is dropped?

No. Data for external table remain intact even if the table is dropped, only metadata is lost when dropping an external table. 

## Q3 : What is the difference between a RC and an ORC file format?

Both RC and ORC are designed to achieve the following goals, ORC is the next iteration of RC file:

1. fast data loading 
2. fast query processing 
3. highly efficient storage space utilization
4. strong adaptivity to highly dynamic workload patterns

The RCFile splits data horizontally into row groups. For example, rows 1 to 100 are stored in one group and rows 101 to 200 in the next and so on. One or several groups are stored in a HDFS file. The RCFile saves the row group data in a columnar format. So instead of storing row one then row two, it stores column one across all rows then column two across all rows and so on.

For ORC format, each file with the columnar layout is optimised for compression and skipping of data/columns to reduce read and decompression load. ORC goes beyond RCFile and uses specific encoders for different column data types to improve compression further, e.g. variable length compression on integers. ORC introduces a lightweight indexing that enables skipping of complete blocks of rows that do not match a query. It comes with basic statistics — min, max, sum, and count — on columns.

## Q4 : In which case, you would use Parquet file?

Parquet Files are yet another columnar file format that originated from Hadoop creator Doug Cutting’s Trevni project. Like RC and ORC, Parquet enjoys compression and query performance benefits, and is generally slower to write than non-columnar file formats. However, unlike RC and ORC files Parquet serdes support limited schema evolution. In Parquet, new columns can be added at the end of the structure.

Reference: [http://www.inquidia.com/news-and-info/hadoop-file-formats-its-not-just-csv-anymore](http://www.inquidia.com/news-and-info/hadoop-file-formats-its-not-just-csv-anymore)

If query performance against the data is most important (at a cost of slower writes), and data schema might change, Parquet should be chosen. 

## Q5 : What are the compression types allowed while using Parquet file?

Snappy, GZIP, deflate, BZIP2; currently Snappy by default

# Lab3 Log
```
Script started on Sun 20 Sep 2015 08:34:05 PM PDT
Last login: Mon Sep 21 03:35:12 2015 from *.*.*.*
     ___   _	    __	 __   ____	      __
    / _ \ (_)___ _ / /	/ /_ / __/____ ___ _ / /___
   / , _// // _ `// _ \/ __/_\ \ / __// _ `// // -_)
  /_/|_|/_/ \_, //_//_/\__//___/ \__/ \_,_//_/ \__/
	   /___/

Welcome to a virtual machine image brought to you by RightScale!

[root@ip-10-109-181-29 ~]# hive

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
WARNING: Hive CLI is deprecated and migration to Beeline is recommended.
hive> CREATE TABLE Web_Session_Log
    > (DATETIME varchar(500),
    > USERID varchar(500),
    > SESSIONID varchar(500),
    > PRODUCTID varchar(500),
    > REFERERURL varchar(500))
    > row format delimited
    > fields terminated by '\t'
    > stored as textfile;
OK
Time taken: 2.068 seconds
[root@ip-10-109-181-29 ~]# wget -O /mnt/weblog_lab.csv https://s3.amazonaws.com/ucbdatasciencew205/labs/weblog_lab.csv
--2015-09-21 03:41:13--  https://s3.amazonaws.com/ucbdatasciencew205/labs/weblog_lab.csv
Resolving s3.amazonaws.com... 54.231.12.104
Connecting to s3.amazonaws.com|54.231.12.104|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 5192992 (5.0M) [application/octet-stream]
Saving to: “/mnt/weblog_lab.csv”

100%[=====================================================================================================================================================================================================>] 5,192,992	 --.-K/s   in 0.1s

2015-09-21 03:41:13 (49.3 MB/s) - “/mnt/weblog_lab.csv” saved [5192992/5192992]

[root@ip-10-109-181-29 ~]# hive

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
WARNING: Hive CLI is deprecated and migration to Beeline is recommended.
hive> LOAD DATA LOCAL INPATH '/mnt/weblog_lab.csv'
    > OVERWRITE INTO TABLE Web_Session_Log;
Loading data to table default.web_session_log
Table default.web_session_log stats: [numFiles=1, numRows=0, totalSize=5192992, rawDataSize=0]
OK
Time taken: 2.873 seconds
hive> CREATE TABLE Web_Session_Log_RC
    > (DATETIME varchar(500),
    > USERID varchar(500),
    > SESSIONID varchar(500),
    > PRODUCTID varchar(500),
    > REFERERURL varchar(500))
    > row format delimited
    > fields terminated by '\t'
    > STORED AS RCFILE;
OK
Time taken: 0.679 seconds
hive> INSERT OVERWRITE TABLE Web_Session_Log_RC
    > select * from Web_Session_Log;
Query ID = root_20150921034343_76dc9385-5268-48f6-b58f-1480938c4ae1
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Job running in-process (local Hadoop)
2015-09-21 03:43:58,759 Stage-1 map = 0%,  reduce = 0%
2015-09-21 03:44:01,776 Stage-1 map = 100%,  reduce = 0%
Ended Job = job_local1531467897_0001
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: file:/user/hive/warehouse/web_session_log_rc/.hive-staging_hive_2015-09-21_03-43-53_803_1866694158927763631-1/-ext-10000
Loading data to table default.web_session_log_rc
Table default.web_session_log_rc stats: [numFiles=1, numRows=40002, totalSize=5022488, rawDataSize=4952980]
MapReduce Jobs Launched:
Stage-Stage-1:	HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 9.088 seconds
hive> SELECT SESSIONID,count(*) as count from Web_Session_Log_RC GROUP BY SESSIONID ORDER BY count;
Query ID = root_20150921034545_74c3dd17-a836-44ba-9fda-8ef330e211c7
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2015-09-21 03:45:27,788 Stage-1 map = 0%,  reduce = 0%
2015-09-21 03:45:32,858 Stage-1 map = 100%,  reduce = 0%
2015-09-21 03:45:38,088 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1393157160_0002
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2015-09-21 03:45:39,572 Stage-2 map = 0%,  reduce = 0%
2015-09-21 03:45:41,688 Stage-2 map = 100%,  reduce = 0%
2015-09-21 03:45:42,693 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_local1259669617_0003
MapReduce Jobs Launched:
Stage-Stage-1:	HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-2:	HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
;+.ASPXAUTH=002N55Q9LOUGP3D3Y	1
;+.ASPXAUTH=ZZYR86XFZ8I6CHC8X	1
;+.ASPXAUTH=ZZW94TS26IYDCLOJX	1
... 
...
;+.ASPXAUTH=0051BVNH3NK4VQJHJ	1
;+.ASPXAUTH=0049F2TBHRUBXCKGM	1
sessionid	2
Time taken: 17.665 seconds, Fetched: 40001 row(s)
hive> CREATE TABLE Web_Session_Log_Managed
    > (DATETIME varchar(500),
    > USERID varchar(500),
    > SESSIONID varchar(500),
    > PRODUCTID varchar(500),
    > REFERERURL varchar(500))
    > row format delimited
    > fields terminated by '\t'
    > stored as textfile;
OK
Time taken: 0.199 seconds
hive> CREATE EXTERNAL TABLE IF NOT EXIST Web_Session_Log_External
    > (DATETIME varchar(500),
    > USERID varchar(500),
    > SESSIONID varchar(500),
    > PRODUCTID varchar(500),
    > REFERERURL varchar(500))
    > row format delimited
    > fields terminated by '\t'
    > stored as textfile
    > LOCATION14GK13GK12GK11GK10GK9GK8GK7GKLOCATION '/hiveweblog';
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:4686)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2355)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1579)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1057)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:393)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1110)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1158)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1047)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1037)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:29 missing KW_EXISTS at 'EXIST' near 'EXIST' in create table statement
line 1:35 cannot recognize input near 'Web_Session_Log_External' '(' 'DATETIME' in create table statement
hive> CREATE EXTERNAL TABLE IF NOT EXISTS Web_Session_Log_External
    > (DATETIME varchar(500),
    > USERID varchar(500),
    > SESSIONID varchar(500),
    > PRODUCTID varchar(500),
    > REFERERURL varchar(500))
    > row format delimited
    > fields terminated by '\t'
    > stored as textfile
    > LOCATION14GK13GK12GK11GK10GK9GK8GK7GKLOCATION '/hiveweblog';
OK
Time taken: 0.106 seconds
hive> CREATE TABLE ORCFileFormatExample
    > (DATETIME varchar(500),
    > USERID varchar(500),
    > SESSIONID varchar(500),
    > PRODUCTID varchar(500),
    > REFERERURL varchar(500))
    >  COMMENT 'This is the Web Session Log data'
    >  ROW FORMAT DELIMITED
    >  FIELDS TERMINATED BY '\t'
    >  STORED AS ORC tblproperties ("orc.compress"="GLIB");
OK
Time taken: 0.163 seconds
hive> CREATE TABLE ParqFileFormatExample(
    > DATETIME varchar(500),
    > USERID varchar(500),
    > SESSIONID varchar(500),
    > PRODUCTID varchar(500),
    > REFERERURL varchar(500))
    >  COMMENT 'This is the Web Session Log data'
    >  ROW FORMAT DELIMITED
    >  FIELDS TERMINATED BY '\t'
    >  STORED AS parquet;
OK
Time taken: 0.14 seconds
hive> SELECT REFERERURL,count(*) as count from Web_Session_Log_RC GROUP BY REFERERURL ORDER BY count WHERE DATETIME>='2015-01-01';130G129G;K129G128G;K128G127G;K127G126G;K126G125G;K125G124G;K124G123G;K123G122G;K122G121G;K121G120G;K120G119G;K119G118G;K118G117G;K117G116G;K116G115G;K115G114G;K114G113G;K113G112G;K112G111G;K111G110G;K110G109G;K109G108G;K108G107G;K107G106G;K106G105G;K105G104G;K104G103G;K103G102G;K102G101G;K101G102G
Query ID = root_20150921040202_29e8e266-0dd3-4199-ba46-cb947be73162
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2015-09-21 04:02:38,410 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1552098508_0004
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2015-09-21 04:02:39,814 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_local1668584478_0005
MapReduce Jobs Launched:
Stage-Stage-1:	HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-2:	HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
refererurl	2
http://www.google.com	3878
http://www.ebay.com	3943
http://www.abc.com	3951
http://www.yahoo.com	3987
http://www.xyz.com	3992
http://www.homeshop18.com	4026
http://www.facebook.com 4035
http://www.shophealthy.com	4050
http://www.amazon.com	4065
http://www.twitter.com	4073
Time taken: 3.012 seconds, Fetched: 11 row(s)
hive> [root@ip-10-109-181-29 ~]# logout
'rxvt-unicode-256color': unknown terminal type.
Connection to ec2-54-237-2-23.compute-1.amazonaws.com closed.
0;yiran@yiran-vizio: ~/Documents/mids/W205/awsyiran@yiran-vizio:~/Documents/mids/W205/aws$ exit
exit

Script done on Sun 20 Sep 2015 09:03:01 PM PDT  
  
```